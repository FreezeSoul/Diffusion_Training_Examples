{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b494ed88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "#全局变量\n",
    "hub_token = open('/root/hub_token.txt').read().strip()\n",
    "repo_id = 'lansinuote/diffusion.3.dream_boothimages'\n",
    "push_to_hub = True\n",
    "checkpoint = 'CompVis/stable-diffusion-v1-4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "315ad307",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pushing split train to the Hub.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b1494099134efd87f9a8788e3f4755",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ff2b1be6b94837865668028ecb9e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pushing dataset shards to the dataset hub:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lansinuote--diffusion.3.dream_boothimages-73d13cf74f9b46f0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--diffusion.3.dream_boothimages-73d13cf74f9b46f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63591b28c0014f00b2646467114799c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626ecd5927634af78869582b08bb7bc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/5.59M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8511758c96ed43f6a3ef9b8959af6cec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/5 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--diffusion.3.dream_boothimages-73d13cf74f9b46f0/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image'],\n",
       "     num_rows: 5\n",
       " }),\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=2469x2558>})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "import torchvision\n",
    "import PIL.Image\n",
    "\n",
    "\n",
    "def get_dataset():\n",
    "    images = [{\n",
    "        'image': PIL.Image.open('images/%d.jpeg' % i)\n",
    "    } for i in range(5)]\n",
    "\n",
    "    dataset = Dataset.from_list(images)\n",
    "\n",
    "    return DatasetDict({'train': dataset})\n",
    "\n",
    "\n",
    "if push_to_hub:\n",
    "    dataset = get_dataset()\n",
    "    dataset.push_to_hub(repo_id=repo_id, token=hub_token)\n",
    "\n",
    "#直接使用我处理好的数据集\n",
    "dataset = load_dataset(path=repo_id, split='train')\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2af94061",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image'],\n",
       "     num_rows: 5\n",
       " }),\n",
       " {'image': tensor([[[ 0.7725,  0.7725,  0.7725,  ...,  0.7647,  0.7647,  0.7725],\n",
       "           [ 0.7804,  0.7882,  0.7804,  ...,  0.7804,  0.7569,  0.7490],\n",
       "           [ 0.7725,  0.7804,  0.7804,  ...,  0.7647,  0.7725,  0.7647],\n",
       "           ...,\n",
       "           [ 0.7176,  0.7020,  0.7176,  ...,  0.7176,  0.7020,  0.7020],\n",
       "           [ 0.7098,  0.7098,  0.7020,  ...,  0.7020,  0.6941,  0.7098],\n",
       "           [ 0.7098,  0.7176,  0.7176,  ...,  0.6941,  0.7098,  0.7020]],\n",
       "  \n",
       "          [[-0.1529, -0.1529, -0.1529,  ..., -0.1451, -0.1373, -0.1216],\n",
       "           [-0.1451, -0.1373, -0.1451,  ..., -0.1373, -0.1529, -0.1529],\n",
       "           [-0.1529, -0.1451, -0.1451,  ..., -0.1373, -0.1373, -0.1373],\n",
       "           ...,\n",
       "           [-0.2471, -0.2627, -0.2471,  ..., -0.2235, -0.2392, -0.2392],\n",
       "           [-0.2392, -0.2471, -0.2549,  ..., -0.2392, -0.2471, -0.2314],\n",
       "           [-0.2549, -0.2471, -0.2471,  ..., -0.2471, -0.2314, -0.2392]],\n",
       "  \n",
       "          [[-0.6392, -0.6392, -0.6392,  ..., -0.6549, -0.6471, -0.6314],\n",
       "           [-0.6314, -0.6235, -0.6314,  ..., -0.6235, -0.6471, -0.6471],\n",
       "           [-0.6392, -0.6314, -0.6314,  ..., -0.6314, -0.6314, -0.6392],\n",
       "           ...,\n",
       "           [-0.6549, -0.6706, -0.6549,  ..., -0.6392, -0.6549, -0.6627],\n",
       "           [-0.6549, -0.6549, -0.6627,  ..., -0.6549, -0.6627, -0.6471],\n",
       "           [-0.6627, -0.6549, -0.6549,  ..., -0.6549, -0.6392, -0.6549]]])})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "#数据增强\n",
    "compose = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(\n",
    "        512, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    torchvision.transforms.RandomCrop(512),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    image = [compose(i) for i in data['image']]\n",
    "    return {'image': image}\n",
    "\n",
    "\n",
    "dataset = dataset.with_transform(f)\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c07c2a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, torch.Size([1, 3, 512, 512]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(datas):\n",
    "    image = [i['image'] for i in datas]\n",
    "    return torch.stack(image, dim=0)\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=collate_fn)\n",
    "\n",
    "len(loader), next(iter(loader)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0459d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder 12306.048\n",
      "vae 8365.3863\n",
      "unet 85952.0964\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPTextModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "encoder = CLIPTextModel.from_pretrained(checkpoint, subfolder='text_encoder')\n",
    "vae = AutoencoderKL.from_pretrained(checkpoint, subfolder='vae')\n",
    "unet = UNet2DConditionModel.from_pretrained(checkpoint, subfolder='unet')\n",
    "\n",
    "vae.requires_grad_(False)\n",
    "encoder.requires_grad_(False)\n",
    "\n",
    "\n",
    "def print_model_size(name, model):\n",
    "    print(name, sum(i.numel() for i in model.parameters()) / 10000)\n",
    "\n",
    "\n",
    "print_model_size('encoder', encoder)\n",
    "print_model_size('vae', vae)\n",
    "print_model_size('unet', unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f4b455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CLIPTokenizer(name_or_path='CompVis/stable-diffusion-v1-4', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}),\n",
       " DDPMScheduler {\n",
       "   \"_class_name\": \"DDPMScheduler\",\n",
       "   \"_diffusers_version\": \"0.12.1\",\n",
       "   \"beta_end\": 0.012,\n",
       "   \"beta_schedule\": \"scaled_linear\",\n",
       "   \"beta_start\": 0.00085,\n",
       "   \"clip_sample\": false,\n",
       "   \"num_train_timesteps\": 1000,\n",
       "   \"prediction_type\": \"epsilon\",\n",
       "   \"set_alpha_to_one\": false,\n",
       "   \"skip_prk_steps\": true,\n",
       "   \"steps_offset\": 1,\n",
       "   \"trained_betas\": null,\n",
       "   \"variance_type\": \"fixed_small\"\n",
       " },\n",
       " AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     lr: 5e-06\n",
       "     maximize: False\n",
       "     weight_decay: 0.01\n",
       " ),\n",
       " MSELoss())"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint,\n",
    "                                          subfolder='tokenizer',\n",
    "                                          use_fast=False)\n",
    "\n",
    "scheduler = DDPMScheduler.from_pretrained(checkpoint, subfolder='scheduler')\n",
    "\n",
    "optimizer = torch.optim.AdamW(unet.parameters(),\n",
    "                              lr=5e-6,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              weight_decay=0.01,\n",
    "                              eps=1e-8)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "tokenizer, scheduler, optimizer, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b3effea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0034, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss(data):\n",
    "    device = data.device\n",
    "\n",
    "    #只需要input ids就可以了\n",
    "    #全程都是一句话,其实只需要编码一次即可\n",
    "    #77 = tokenizer.model_max_length\n",
    "    input_ids = tokenizer('a photo of sks dog',\n",
    "                          truncation=True,\n",
    "                          padding='max_length',\n",
    "                          max_length=77,\n",
    "                          return_tensors='pt')['input_ids']\n",
    "\n",
    "    #编码文字,由于encoder不训练,其实这一步也可以只运算一次\n",
    "    #[1, 77] -> [1, 77, 768]\n",
    "    out_encoder = encoder(input_ids.to(device))[0]\n",
    "\n",
    "    #vae计算特征图\n",
    "    #[1, 3, 512, 512] -> [1, 4, 64, 64]\n",
    "    out_vae = vae.encode(data).latent_dist.sample().detach()\n",
    "\n",
    "    #0.18215 = vae.config.scaling_factor\n",
    "    out_vae = out_vae * 0.18215\n",
    "\n",
    "    #随机噪声\n",
    "    #[1, 4, 64, 64]\n",
    "    noise = torch.randn_like(out_vae)\n",
    "\n",
    "    #随机噪声步\n",
    "    #1000 = scheduler.config.num_train_timesteps\n",
    "    #1 = b\n",
    "    noise_step = torch.randint(0, 1000, (1, ), device=device).long()\n",
    "\n",
    "    #添加噪声\n",
    "    #[1, 4, 64, 64]\n",
    "    out_vae_noise = scheduler.add_noise(out_vae, noise, noise_step)\n",
    "\n",
    "    #unet从噪声图中把噪声计算出来\n",
    "    #[1, 4, 64, 64],[1, 77, 768] -> [1, 4, 64, 64]\n",
    "    out_unet = unet(out_vae_noise, noise_step, out_encoder).sample\n",
    "\n",
    "    return criterion(out_unet, noise)\n",
    "\n",
    "\n",
    "get_loss(torch.randn(1, 3, 512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13a8c245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/code/Diffusers/3.dream booth/./save is already a clone of https://huggingface.co/lansinuote/diffusion.3.dream_boothimages. Make sure you pull the latest changes with `repo.git_pull()`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 1.2774939220398664\n",
      "20 8.995061814552173\n",
      "40 7.317525086225942\n",
      "60 7.997788276174106\n",
      "80 8.736398497479968\n",
      "100 8.037149989278987\n",
      "120 6.951517629553564\n",
      "140 8.025313258636743\n",
      "160 7.182113338029012\n",
      "180 5.828842404880561\n",
      "200 4.075685135438107\n",
      "220 4.641587202670053\n",
      "240 8.873134223162197\n",
      "260 3.9080546323675662\n",
      "280 6.689343460020609\n",
      "300 5.815101726853754\n",
      "320 6.1464903430314735\n",
      "340 5.386142439674586\n",
      "360 4.5272802241379395\n",
      "380 4.679458085098304\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da99f321bf5e45d289f32fbfd5b57de4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 16 files:   0%|          | 0/16 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1957d67d1d48129f43d8a78e571888",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file unet/diffusion_pytorch_model.bin:   0%|          | 32.0k/3.20G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec83f461e90646649aa005a15ab36b72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file vae/diffusion_pytorch_model.bin:   0%|          | 32.0k/319M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5ae711a59754287b743f44b660185be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file safety_checker/pytorch_model.bin:   0%|          | 32.0k/1.13G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eab2da8f395b4d599df367b475af93c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file text_encoder/pytorch_model.bin:   0%|          | 32.0k/470M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "remote: Scanning LFS files for validity...\u001b[K\n",
      "remote: LFS file scan complete.\u001b[K\n",
      "To https://user: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@huggingface.co/lansinuote/diffusion.3.dream_boothimages\n",
      "   e15cbf4..9451540  main -> main\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "from huggingface_hub import Repository, create_repo\n",
    "\n",
    "\n",
    "def train():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    unet.to(device)\n",
    "    vae.to(device)\n",
    "    encoder.to(device)\n",
    "    unet.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    for epoch in range(400):\n",
    "        for i, data in enumerate(loader):\n",
    "            loss = get_loss(data.to(device))\n",
    "\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(unet.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        if epoch % 20 == 0:\n",
    "            print(epoch, loss_sum)\n",
    "            loss_sum = 0\n",
    "\n",
    "    DiffusionPipeline.from_pretrained(\n",
    "        checkpoint, unet=unet, text_encoder=encoder).save_pretrained('./save')\n",
    "\n",
    "\n",
    "if push_to_hub:\n",
    "    create_repo(repo_id, exist_ok=True, token=hub_token)\n",
    "    repo = Repository('./save', clone_from=repo_id, token=hub_token)\n",
    "    train()\n",
    "    repo.push_to_hub()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

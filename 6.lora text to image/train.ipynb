{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "315ad307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration lansinuote--diffusion.4.text_to_image-c3c926d45403802a\n",
      "Found cached dataset parquet (/root/.cache/huggingface/datasets/lansinuote___parquet/lansinuote--diffusion.4.text_to_image-c3c926d45403802a/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'input_ids'],\n",
       "     num_rows: 833\n",
       " }),\n",
       " {'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=1280x1280>,\n",
       "  'input_ids': [49406,\n",
       "   320,\n",
       "   3610,\n",
       "   539,\n",
       "   320,\n",
       "   1901,\n",
       "   9528,\n",
       "   593,\n",
       "   736,\n",
       "   3095,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407]})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "#直接使用我处理好的数据集\n",
    "dataset = load_dataset(path='lansinuote/diffusion.4.text_to_image',\n",
    "                       split='train')\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2af94061",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['image', 'input_ids'],\n",
       "     num_rows: 833\n",
       " }),\n",
       " {'image': tensor([[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "          [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           ...,\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "           [1., 1., 1.,  ..., 1., 1., 1.]]]),\n",
       "  'input_ids': [49406,\n",
       "   320,\n",
       "   3610,\n",
       "   539,\n",
       "   320,\n",
       "   1901,\n",
       "   9528,\n",
       "   593,\n",
       "   736,\n",
       "   3095,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407,\n",
       "   49407]})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision\n",
    "\n",
    "#数据增强\n",
    "compose = torchvision.transforms.Compose([\n",
    "    torchvision.transforms.Resize(\n",
    "        512, interpolation=torchvision.transforms.InterpolationMode.BILINEAR),\n",
    "    torchvision.transforms.CenterCrop(512),\n",
    "    torchvision.transforms.RandomHorizontalFlip(),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize([0.5], [0.5]),\n",
    "])\n",
    "\n",
    "\n",
    "def f(data):\n",
    "    image = [compose(i) for i in data['image']]\n",
    "    return {'image': image, 'input_ids': data['input_ids']}\n",
    "\n",
    "\n",
    "dataset = dataset.with_transform(f)\n",
    "\n",
    "dataset, dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c07c2a12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(833,\n",
       " {'image': tensor([[[[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "           [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]],\n",
       "  \n",
       "           [[1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            ...,\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.],\n",
       "            [1., 1., 1.,  ..., 1., 1., 1.]]]]),\n",
       "  'input_ids': tensor([[49406,   320,  1579,   537,   736, 11781,  4668,   593,  1449,  3095,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
       "           49407, 49407, 49407, 49407, 49407, 49407, 49407]])})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def collate_fn(datas):\n",
    "    image = torch.stack([i['image'] for i in datas], dim=0)\n",
    "    input_ids = torch.LongTensor([i['input_ids'] for i in datas])\n",
    "\n",
    "    return {'image': image, 'input_ids': input_ids}\n",
    "\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset,\n",
    "                                     batch_size=1,\n",
    "                                     shuffle=True,\n",
    "                                     collate_fn=collate_fn)\n",
    "\n",
    "len(loader), next(iter(loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0459d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder 12306.048\n",
      "vae 8365.3863\n",
      "unet 85952.0964\n"
     ]
    }
   ],
   "source": [
    "from transformers.models.clip.modeling_clip import CLIPTextModel\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "\n",
    "checkpoint = 'runwayml/stable-diffusion-v1-5'\n",
    "\n",
    "encoder = CLIPTextModel.from_pretrained(checkpoint, subfolder='text_encoder')\n",
    "vae = AutoencoderKL.from_pretrained(checkpoint, subfolder='vae')\n",
    "unet = UNet2DConditionModel.from_pretrained(checkpoint, subfolder='unet')\n",
    "\n",
    "vae.requires_grad_(False)\n",
    "encoder.requires_grad_(False)\n",
    "unet.requires_grad_(False)\n",
    "\n",
    "\n",
    "def print_model_size(name, model):\n",
    "    print(name, sum(i.numel() for i in model.parameters()) / 10000)\n",
    "\n",
    "\n",
    "print_model_size('encoder', encoder)\n",
    "print_model_size('vae', vae)\n",
    "print_model_size('unet', unet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269ee2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "down_blocks.0.attentions.0.transformer_blocks.0.attn1.processor 320 None\n",
      "down_blocks.0.attentions.0.transformer_blocks.0.attn2.processor 320 768\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn1.processor 320 None\n",
      "down_blocks.0.attentions.1.transformer_blocks.0.attn2.processor 320 768\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn1.processor 640 None\n",
      "down_blocks.1.attentions.0.transformer_blocks.0.attn2.processor 640 768\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn1.processor 640 None\n",
      "down_blocks.1.attentions.1.transformer_blocks.0.attn2.processor 640 768\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor 1280 None\n",
      "down_blocks.2.attentions.0.transformer_blocks.0.attn2.processor 1280 768\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn1.processor 1280 None\n",
      "down_blocks.2.attentions.1.transformer_blocks.0.attn2.processor 1280 768\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn1.processor 1280 None\n",
      "up_blocks.1.attentions.0.transformer_blocks.0.attn2.processor 1280 768\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn1.processor 1280 None\n",
      "up_blocks.1.attentions.1.transformer_blocks.0.attn2.processor 1280 768\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn1.processor 1280 None\n",
      "up_blocks.1.attentions.2.transformer_blocks.0.attn2.processor 1280 768\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn1.processor 640 None\n",
      "up_blocks.2.attentions.0.transformer_blocks.0.attn2.processor 640 768\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn1.processor 640 None\n",
      "up_blocks.2.attentions.1.transformer_blocks.0.attn2.processor 640 768\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn1.processor 640 None\n",
      "up_blocks.2.attentions.2.transformer_blocks.0.attn2.processor 640 768\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn1.processor 320 None\n",
      "up_blocks.3.attentions.0.transformer_blocks.0.attn2.processor 320 768\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn1.processor 320 None\n",
      "up_blocks.3.attentions.1.transformer_blocks.0.attn2.processor 320 768\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn1.processor 320 None\n",
      "up_blocks.3.attentions.2.transformer_blocks.0.attn2.processor 320 768\n",
      "mid_block.attentions.0.transformer_blocks.0.attn1.processor 1280 None\n",
      "mid_block.attentions.0.transformer_blocks.0.attn2.processor 1280 768\n"
     ]
    }
   ],
   "source": [
    "#这个类不想测了,总之就是在做注意力的调整\n",
    "from diffusers.models.attention_processor import LoRAAttnProcessor\n",
    "\n",
    "\n",
    "def set_processors():\n",
    "    processors = {}\n",
    "\n",
    "    #遍历unet的所有层,找出所有有set_processor属性的层,每一个都组装成lora层\n",
    "    for name in unet.attn_processors.keys():\n",
    "\n",
    "        #768 = unet.config.cross_attention_dim\n",
    "        cross_attention_dim = 768\n",
    "        if name.endswith('attn1.processor'):\n",
    "            cross_attention_dim = None\n",
    "\n",
    "        if name.startswith('mid_block'):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "\n",
    "        if name.startswith('up_blocks'):\n",
    "            #取层名字中的第一个数字\n",
    "            #p_blocks.1.attentions.0.transformer_blocks.0.attn1.processor -> 1\n",
    "            block_id = int(name[10])\n",
    "            hidden_size = list(reversed(\n",
    "                unet.config.block_out_channels))[block_id]\n",
    "\n",
    "        if name.startswith('down_blocks'):\n",
    "            #取层名字中的第一个数字\n",
    "            #down_blocks.2.attentions.0.transformer_blocks.0.attn1.processor -> 2\n",
    "            block_id = int(name[12])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "        processors[name] = LoRAAttnProcessor(hidden_size, cross_attention_dim)\n",
    "\n",
    "        print(name, hidden_size, cross_attention_dim)\n",
    "\n",
    "    #把上面组装好的字典,设置到unet的层当中\n",
    "    unet.set_attn_processor(processors)\n",
    "\n",
    "\n",
    "set_processors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2690fbdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_layers = torch.nn.ModuleList(unet.attn_processors.values())\n",
    "\n",
    "len(lora_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6f4b455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CLIPTokenizer(name_or_path='runwayml/stable-diffusion-v1-5', vocab_size=49408, model_max_length=77, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': '<|endoftext|>'}),\n",
       " DDPMScheduler {\n",
       "   \"_class_name\": \"DDPMScheduler\",\n",
       "   \"_diffusers_version\": \"0.15.1\",\n",
       "   \"beta_end\": 0.012,\n",
       "   \"beta_schedule\": \"scaled_linear\",\n",
       "   \"beta_start\": 0.00085,\n",
       "   \"clip_sample\": false,\n",
       "   \"clip_sample_range\": 1.0,\n",
       "   \"dynamic_thresholding_ratio\": 0.995,\n",
       "   \"num_train_timesteps\": 1000,\n",
       "   \"prediction_type\": \"epsilon\",\n",
       "   \"sample_max_value\": 1.0,\n",
       "   \"set_alpha_to_one\": false,\n",
       "   \"skip_prk_steps\": true,\n",
       "   \"steps_offset\": 1,\n",
       "   \"thresholding\": false,\n",
       "   \"trained_betas\": null,\n",
       "   \"variance_type\": \"fixed_small\"\n",
       " },\n",
       " AdamW (\n",
       " Parameter Group 0\n",
       "     amsgrad: False\n",
       "     betas: (0.9, 0.999)\n",
       "     capturable: False\n",
       "     eps: 1e-08\n",
       "     foreach: None\n",
       "     initial_lr: 0.0001\n",
       "     lr: 0.0001\n",
       "     maximize: False\n",
       "     weight_decay: 0.01\n",
       " ),\n",
       " <torch.optim.lr_scheduler.LambdaLR at 0x7f8bafc98ac0>,\n",
       " MSELoss())"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from diffusers import DDPMScheduler\n",
    "from transformers import AutoTokenizer\n",
    "from diffusers.optimization import get_scheduler\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint,\n",
    "                                          subfolder='tokenizer',\n",
    "                                          use_fast=False)\n",
    "\n",
    "scheduler = DDPMScheduler.from_pretrained(checkpoint, subfolder='scheduler')\n",
    "\n",
    "optimizer = torch.optim.AdamW(lora_layers.parameters(),\n",
    "                              lr=1e-4,\n",
    "                              betas=(0.9, 0.999),\n",
    "                              weight_decay=0.01,\n",
    "                              eps=1e-8)\n",
    "\n",
    "scheduler_lr = get_scheduler('cosine',\n",
    "                             optimizer=optimizer,\n",
    "                             num_warmup_steps=0,\n",
    "                             num_training_steps=len(loader) * 400 // 4)\n",
    "\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "tokenizer, scheduler, optimizer, scheduler_lr, criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3effea6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6869, grad_fn=<MseLossBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_loss(data):\n",
    "    device = data['input_ids'].device\n",
    "\n",
    "    #编码文字\n",
    "    #[1, 77] -> [1, 77, 768]\n",
    "    out_encoder = encoder(data['input_ids'])[0]\n",
    "\n",
    "    #vae计算特征图\n",
    "    #[1, 3, 512, 512] -> [1, 4, 64, 64]\n",
    "    out_vae = vae.encode(data['image']).latent_dist.sample()\n",
    "\n",
    "    #0.18215 = vae.config.scaling_factor\n",
    "    out_vae = out_vae * 0.18215\n",
    "\n",
    "    #随机噪声\n",
    "    #[1, 4, 64, 64]\n",
    "    noise = torch.randn_like(out_vae)\n",
    "\n",
    "    #随机噪声步\n",
    "    #1000 = scheduler.config.num_train_timesteps\n",
    "    #1 = b\n",
    "    noise_step = torch.randint(0, 1000, (1, ), device=device).long()\n",
    "\n",
    "    #添加噪声\n",
    "    #[1, 4, 64, 64]\n",
    "    out_vae_noise = scheduler.add_noise(out_vae, noise, noise_step)\n",
    "\n",
    "    #unet从噪声图中把噪声计算出来\n",
    "    #[1, 4, 64, 64],[1, 77, 768] -> [1, 4, 64, 64]\n",
    "    out_unet = unet(out_vae_noise, noise_step, out_encoder).sample\n",
    "\n",
    "    return criterion(out_unet, noise)\n",
    "\n",
    "\n",
    "get_loss({\n",
    "    'image': torch.randn(1, 3, 512, 512),\n",
    "    'input_ids': torch.arange(77).reshape(1, 77)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a8c245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10.703415064956062 9.99984467546043e-05\n",
      "5 56.16534663550556 9.994444932466895e-05\n",
      "10 52.10641818065778 9.981347814466988e-05\n",
      "15 54.88103978987783 9.96057350657239e-05\n",
      "20 53.18660080563859 9.932123063252573e-05\n",
      "25 53.15705438374425 9.896094932338045e-05\n",
      "30 51.69287981011439 9.852521002186447e-05\n",
      "35 54.403384188131895 9.801468428384716e-05\n",
      "40 54.496449730562745 9.742956212449453e-05\n",
      "45 52.372736920719035 9.67718682576756e-05\n",
      "50 53.141422546003014 9.604209018238184e-05\n",
      "55 52.44126057237736 9.524135262330098e-05\n",
      "60 53.340201130195055 9.437002038726624e-05\n",
      "65 52.79343696354772 9.343110856853554e-05\n",
      "70 52.16929884609999 9.242526127767233e-05\n",
      "75 51.336487896653125 9.135402871372808e-05\n",
      "80 52.107932627550326 9.021794149244833e-05\n",
      "85 53.66056731872959 8.902093084685268e-05\n",
      "90 53.038139166950714 8.776378163893685e-05\n",
      "95 52.564585573622026 8.644843137107059e-05\n",
      "100 52.97269177176349 8.5075563402391e-05\n",
      "105 52.06748179736314 8.364992830467555e-05\n",
      "110 52.05431841494283 8.2172432365445e-05\n",
      "115 51.72748707811115 8.064535268264883e-05\n",
      "120 52.86211925785756 7.90695085322459e-05\n",
      "125 52.61736843918334 7.745035284992803e-05\n",
      "130 52.74300374410814 7.578889103293449e-05\n",
      "135 53.85954987225705 7.408768370508576e-05\n",
      "140 50.80803845819901 7.234766588586932e-05\n",
      "145 52.524096080160234 7.05748586055173e-05\n"
     ]
    }
   ],
   "source": [
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "\n",
    "def train():\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    unet.to(device)\n",
    "    vae.to(device)\n",
    "    encoder.to(device)\n",
    "    unet.train()\n",
    "\n",
    "    loss_sum = 0\n",
    "    for epoch in range(150):\n",
    "        for i, data in enumerate(loader):\n",
    "            data['image'] = data['image'].to(device)\n",
    "            data['input_ids'] = data['input_ids'].to(device)\n",
    "            loss = get_loss(data) / 4\n",
    "            loss.backward()\n",
    "\n",
    "            if (epoch * len(loader) + i) % 4 == 0:\n",
    "                torch.nn.utils.clip_grad_norm_(lora_layers.parameters(), 1.0)\n",
    "                optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "                scheduler_lr.step()\n",
    "\n",
    "            loss_sum += loss.item()\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(epoch, loss_sum, optimizer.param_groups[-1]['lr'])\n",
    "            loss_sum = 0\n",
    "\n",
    "    unet.save_attn_procs('./save')\n",
    "\n",
    "\n",
    "train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pt39]",
   "language": "python",
   "name": "conda-env-pt39-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
